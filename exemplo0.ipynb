{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f39d19b-f7b8-4b04-bc33-71a5dc4f4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import slideio\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights, resnet34, ResNet34_Weights, inception_v3, Inception_V3_Weights\n",
    "from torchsummary import summary\n",
    "from random import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from s3fs import S3FileSystem\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import boto3\n",
    "from botocore.client import ClientError\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('utils')\n",
    "from stainlib.augmentation.augmenter import StainAugmentor\n",
    "from stainlib.augmentation.augmenter import HedLighterColorAugmenter\n",
    "from stainlib.augmentation.augmenter import GrayscaleAugmentor\n",
    "from stainlib.utils.plot_utils import _plot_imagegrid\n",
    "from S3FileManager import S3FileManager, S3UploadSync\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01dc3fc6-0d51-405e-8325-d8a1181f43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def fetch_processed_slides(path):\n",
    "    return {x.replace('.pt', '').split('--')[0] for x in os.listdir(path) if '.pt' in x}\n",
    "\n",
    "def fetch_avg_saturation(image):\n",
    "    blurred_image = cv2.GaussianBlur(image, (5,5), 0)\n",
    "    hsv_image = cv2.cvtColor(blurred_image, cv2.COLOR_RGB2HSV)\n",
    "    return np.mean(hsv_image[:,:,1])\n",
    "\n",
    "def get_file_size(file_path):\n",
    "    file_stats = os.stat(file_path)\n",
    "    return file_stats.st_size/(10**9)\n",
    "\n",
    "def num_estimated_patches(scene, stride):\n",
    "    width = scene.size[0]\n",
    "    height = scene.size[1]\n",
    "    num_expected_patches = (math.ceil((height - stride) / stride)) * (math.ceil((width - stride) / stride))\n",
    "    return num_expected_patches\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "def load_feature_extractor_inception_v3():\n",
    "    inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
    "    inception.eval()\n",
    "    # Remove the last two layers and replace the AdaptiveAvgPool2d layer with a Flatten layer\n",
    "    feature_extractor = torch.nn.Sequential(\n",
    "        *list(inception.children())[:-2],\n",
    "        Flatten()\n",
    "    )\n",
    "    feature_extractor = feature_extractor.to(device)\n",
    "    return feature_extractor\n",
    "\n",
    "def load_feature_extractor_resnet50():\n",
    "    resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "    resnet.eval()\n",
    "    feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "    feature_extractor = feature_extractor.to(device)\n",
    "    return feature_extractor\n",
    "\n",
    "def correct_img(patches):  # Expects the patches to come in the cropped size, 299\n",
    "    preprocess = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    t = torch.stack([preprocess(torch.from_numpy(img).permute(2, 0, 1).float() / 255.0) for img in patches])\n",
    "    t = t.to(device)\n",
    "    return t\n",
    "\n",
    "def get_patch_for_size(slide_handler, size):\n",
    "    scene_0 = slide_handler.get_scene(0)\n",
    "    resolutions = scene_0.resolution\n",
    "    if resolutions == (0, 0):\n",
    "        # Resolução esperada nas imagens com problema em microns per pixel\n",
    "        resolution_microns = 0.467\n",
    "        # Convert to meters per pixel\n",
    "        resolution_meters = resolution_microns * 1e-6\n",
    "        resolutions = (resolution_meters, resolution_meters)\n",
    "\n",
    "    patch_size_x = int(size[0]/resolutions[0])\n",
    "    patch_size_y = int(size[1]/resolutions[1])\n",
    "    return (patch_size_x, patch_size_y)\n",
    "\n",
    "def tensor_to_numpy_image(tensor):\n",
    "    # Move tensor to CPU\n",
    "    tensor = tensor.to('cpu')\n",
    "\n",
    "    # Convert tensor to NumPy array\n",
    "    numpy_img = tensor.numpy()\n",
    "\n",
    "    # Transpose channels back to original order\n",
    "    numpy_img = np.moveaxis(numpy_img, [0, 1, 2], [2, 0, 1])\n",
    "\n",
    "    # Denormalize pixel values\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    numpy_img = (numpy_img * std + mean) * 255\n",
    "\n",
    "    cv2.imwrite(\"tensor.png\", numpy_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3225a63f-1941-4c51-a45a-40770236d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_img2(img):\n",
    "    preprocess = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    img = np.array(img)\n",
    "    \n",
    "    t = preprocess(torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0)\n",
    "    t = t.to(device)\n",
    "    \n",
    "    return t\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "        def __init__(self, all_images, transform=None):\n",
    "            self.all_images = all_images\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.all_images)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img = self.all_images[idx]\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ec289c-0f51-4363-a568-f00af9628bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_generator_with_precision_scale(image_path, batch_size, size_to_fetch=(0.0003861256, 0.0003861256),\n",
    "                                         final_size=224, min_saturation=10, aug_type=0):\n",
    "    slide_handler = slideio.open_slide(image_path, 'SVS')\n",
    "    scene = slide_handler.get_scene(0)\n",
    "    actual_size = scene.size\n",
    "    actual_width = actual_size[0]\n",
    "    actual_height = actual_size[1]\n",
    "    patch_size = get_patch_for_size(slide_handler, size_to_fetch)\n",
    "    patch_x = patch_size[0]\n",
    "    patch_y = patch_size[1]\n",
    "    width_to_use = 10*actual_width//patch_x\n",
    "    smaller_image = scene.read_block(size=(width_to_use,0))\n",
    "    copied_image = scene.read_block(size=(width_to_use,0))\n",
    "    smaller_width = np.shape(smaller_image)[1]\n",
    "    smaller_height = np.shape(smaller_image)[0]\n",
    "    smaller_patch_size_x = patch_x*smaller_width//actual_width\n",
    "    smaller_patch_size_y = patch_y*smaller_height//actual_height\n",
    "\n",
    "    positions = []\n",
    "\n",
    "    hed_lighter_aug = HedLighterColorAugmenter()\n",
    "    hed_lighter_aug.randomize()\n",
    "    grayscale_aug = GrayscaleAugmentor()\n",
    "\n",
    "    avg_saturation_is_ok = False\n",
    "    while not avg_saturation_is_ok:\n",
    "        positions = []\n",
    "        all_saturations = []\n",
    "        current_x = 0\n",
    "        current_y = 0\n",
    "        while current_y <= actual_height:\n",
    "            while current_x <= actual_width:\n",
    "                small_current_y = (current_y * smaller_height)//actual_height\n",
    "                small_current_x = (current_x * smaller_width)//actual_width\n",
    "\n",
    "                bottom_border = small_current_y + smaller_patch_size_y\n",
    "                right_border = small_current_x + smaller_patch_size_x\n",
    "\n",
    "                if bottom_border > smaller_height:\n",
    "                    bottom_border = smaller_height\n",
    "                if right_border > smaller_width:\n",
    "                    right_border = smaller_width\n",
    "\n",
    "                sub_image = smaller_image[small_current_y:bottom_border,\n",
    "                                          small_current_x:right_border]\n",
    "                saturation = fetch_avg_saturation(sub_image)\n",
    "\n",
    "                if saturation > min_saturation:\n",
    "                    all_saturations.append(saturation)\n",
    "                    positions.append((current_x, current_y))\n",
    "                current_x += patch_x\n",
    "            current_y += patch_y\n",
    "            current_x = 0\n",
    "        avg_saturation = np.mean(all_saturations)\n",
    "        if avg_saturation < min_saturation * 3:\n",
    "            min_saturation = min_saturation / 2\n",
    "        else:\n",
    "            avg_saturation_is_ok = True\n",
    "\n",
    "    final_img_batch = []\n",
    "    normalized_pos_batch = []\n",
    "    for i, position in enumerate(positions):\n",
    "        try:\n",
    "            if len(final_img_batch) == batch_size:\n",
    "                final_img_batch = []\n",
    "                normalized_pos_batch = []\n",
    "            x1 = position[0]\n",
    "            y1 = position[1]\n",
    "            width = min(patch_x, actual_width - x1)\n",
    "            height = min(patch_y, actual_height - y1)\n",
    "            normalized_position = (x1//patch_x, y1//patch_y)\n",
    "            normalized_pos_batch.append(normalized_position)\n",
    "\n",
    "            sub_image = scene.read_block((x1, y1, width, height), (final_size, final_size))\n",
    "            # aug_type == 0: Sem augmentation\n",
    "            if aug_type == 1:  # Augmentation bgr\n",
    "                sub_image = cv2.cvtColor(sub_image, cv2.COLOR_RGB2BGR)\n",
    "            elif aug_type == 2:  # Augmentation grayscale\n",
    "                grayscale_aug.fit(sub_image)\n",
    "                sub_image = grayscale_aug.pop()\n",
    "            elif aug_type == 3:  # Tudo embaralhado usando Hematoxylin and Eosin intensity\n",
    "                hed_lighter_aug.randomize()\n",
    "                sub_image = hed_lighter_aug.transform(sub_image)\n",
    "            final_img_batch.append(sub_image)\n",
    "            if len(final_img_batch) == batch_size or i == len(positions) - 1:\n",
    "                yield normalized_pos_batch, np.array(final_img_batch)\n",
    "        except BaseException as e:\n",
    "            print('erro no yield: ' + str(e), image_path)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064e4ab0-e6e0-4167-ba82-f1c91b877de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_feature_array(model, patches):\n",
    "    if device != torch.device('cpu'):\n",
    "        patches = patches.to(device)\n",
    "    raw_output = model(patches)\n",
    "    main_size = raw_output.shape[1]\n",
    "    out_np = raw_output.cpu().detach()\n",
    "    return torch.reshape(out_np, [-1, main_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa013e66-fb08-42bd-b408-ab20cde5487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tensor_from_slide(slide_path, aug_type):\n",
    "    global feat_gen, BATCH_SIZE\n",
    "    patch_gen = patch_generator_with_precision_scale(slide_path, BATCH_SIZE, aug_type=aug_type)\n",
    "#     patch_gen = patch_analyzer_with_precision_scale(slide_path, 0.5, BATCH_SIZE, 1, aug_type=aug_type)\n",
    "    feat_arrays = []\n",
    "    for _, patch_batch in patch_gen:\n",
    "        batch_patches_t = correct_img(patch_batch)\n",
    "        new_feats = fetch_feature_array(feat_gen, batch_patches_t)\n",
    "        feat_arrays.append(new_feats)\n",
    "#     patch_gen = patch_analyzer_with_precision_scale(slide_path, 0.1, BATCH_SIZE, 0.5, aug_type=aug_type)\n",
    "#     for _, patch_batch in patch_gen:\n",
    "#         batch_patches_t = correct_img(patch_batch)\n",
    "#         new_feats = fetch_feature_array(feat_gen, batch_patches_t)\n",
    "#         feat_arrays.append(new_feats)\n",
    "    final_tensor = torch.vstack(feat_arrays)\n",
    "    return final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530a798e-7dde-4690-ab2c-fb911e626e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_gen = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "feat_gen.eval()\n",
    "feat_gen = feat_gen.to(device)\n",
    "num_ftrs = feat_gen.fc.in_features\n",
    "feat_gen.fc = nn.Linear(num_ftrs, 3)\n",
    "feat_gen = torch.nn.Sequential(*list(feat_gen.children())[:-1])\n",
    "# feat_gen.load_state_dict(torch.load(r'patches\\resnet50_98_all_layers2.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8af3eb-6995-45e7-b4a1-95e7e82594bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 375 ms, total: 1.41 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SIZE_MAX = 0.40 * 1024 * 1024 * 1024 # 400Mb\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "def find_svs_files(directory):\n",
    "    return glob.glob(directory + '/**/*.svs', recursive=True)\n",
    "\n",
    "folder = 'lusc'\n",
    "bucket_name = 'oncodata-datasources/tcga/lung/' + folder\n",
    "directory = '/tmp/train-data'\n",
    "\n",
    "file_manager = S3FileManager(bucket_name=bucket_name, local_dir=directory)\n",
    "\n",
    "svs_files = find_svs_files(directory)\n",
    "\n",
    "input_slides = [{'dataset': folder,\n",
    "                 'image_path': path} for path in svs_files]\n",
    "\n",
    "output_folder = '/tmp/preprocessing/resnetmultizoomlusc'\n",
    "create_folder(output_folder)\n",
    "processed_images = fetch_processed_slides(output_folder)\n",
    "#print('PROCESSED IMAGES', processed_images)\n",
    "\n",
    "#feat_gen = load_feature_extractor_resnet50() #load_feature_extractor_densenet() #load_feature_extractor_resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf6752b9-d01d-4575-9c61-9056ab9e929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-33-4586-11A-01-TS1.b34fae7a-c25e-494b-a9a9-f97157a133c9--hechaos__lusc.pt\n",
      "TCGA-33-4586-11A-01-TS1.b34fae7a-c25e-494b-a9a9-f97157a133c9--noaug__lusc.pt\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/preprocessing/resnetmultizoomlusc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ef6d3d-b0bc-4a44-9cc3-501a5341e8c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n",
      "checkpoint2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1608 [02:08<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint2\n",
      "CPU times: user 1min 57s, sys: 1.29 s, total: 1min 59s\n",
      "Wall time: 2min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aug_array = ['noaug', 'bgr', 'gray', 'hechaos']\n",
    "found = False\n",
    "\n",
    "for image_data in tqdm(input_slides):\n",
    "    dataset = image_data['dataset']\n",
    "    image_path = image_data['image_path']\n",
    "    image_name = os.path.split(image_path)[-1].replace('.svs', '')\n",
    "\n",
    "    #print('WILL PROCESS', image_name)\n",
    "    if image_name in processed_images or file_manager.get_file_size(image_path) > SIZE_MAX:\n",
    "        # print('Skipping', image_name, file_manager.get_file_size(image_path))\n",
    "        continue\n",
    "    else:\n",
    "        for aug_type in [0, 3]:\n",
    "            try:\n",
    "                output_name = '%s--%s__%s.pt' % (image_name, aug_array[aug_type], dataset)\n",
    "                output_name = os.path.join(output_folder, output_name)\n",
    "                featureset = generate_tensor_from_slide(image_path, aug_type)\n",
    "                torch.save(featureset, output_name)\n",
    "            except BaseException as e:\n",
    "                print(e, image_name)\n",
    "                pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "876882f0-88b7-4285-af98-64d5e48129c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# list the number of files inside /tmp/train-data\n",
    "print(len(os.listdir(directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c06d289-3b4d-4a23-a9a2-2052db573728",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3syncher = S3UploadSync('oncodata-sagemaker-shared', 'code', 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0bbbc2d-e306-49cc-a71b-0d02b5f10fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path:  model_def.py code/model_def.py testing/model_def.py\n",
      "file_path:  train_pytorch_smdataparallel_mnist.py code/train_pytorch_smdataparallel_mnist.py testing/train_pytorch_smdataparallel_mnist.py\n",
      "file_path:  train_dsmil.py code/train_dsmil.py testing/train_dsmil.py\n",
      "file_path:  create_heatmaps.ipynb code/create_heatmaps.ipynb testing/create_heatmaps.ipynb\n",
      "file_path:  README.md code/README.md testing/README.md\n",
      "file_path:  requirements.txt code/requirements.txt testing/requirements.txt\n",
      "file_path:  roc_stomach.png code/roc_stomach.png testing/roc_stomach.png\n",
      "file_path:  seed_5708.pt code/seed_5708.pt testing/seed_5708.pt\n",
      "file_path:  separate_jsons_by_patient.ipynb code/separate_jsons_by_patient.ipynb testing/separate_jsons_by_patient.ipynb\n",
      "file_path:  stomach_json_test.json code/stomach_json_test.json testing/stomach_json_test.json\n",
      "file_path:  stomach_json_train.json code/stomach_json_train.json testing/stomach_json_train.json\n",
      "file_path:  stomach_json.json code/stomach_json.json testing/stomach_json.json\n",
      "file_path:  train_dsmil.ipynb code/train_dsmil.ipynb testing/train_dsmil.ipynb\n",
      "file_path:  vits.py code/vits.py testing/vits.py\n",
      "file_path:  model_def-checkpoint.py code/.ipynb_checkpoints/model_def-checkpoint.py testing/.ipynb_checkpoints/model_def-checkpoint.py\n",
      "file_path:  train_pytorch_smdataparallel_mnist-checkpoint.py code/.ipynb_checkpoints/train_pytorch_smdataparallel_mnist-checkpoint.py testing/.ipynb_checkpoints/train_pytorch_smdataparallel_mnist-checkpoint.py\n",
      "file_path:  create_heatmaps-checkpoint.ipynb code/.ipynb_checkpoints/create_heatmaps-checkpoint.ipynb testing/.ipynb_checkpoints/create_heatmaps-checkpoint.ipynb\n",
      "file_path:  separate_jsons_by_patient-checkpoint.ipynb code/.ipynb_checkpoints/separate_jsons_by_patient-checkpoint.ipynb testing/.ipynb_checkpoints/separate_jsons_by_patient-checkpoint.ipynb\n",
      "file_path:  train_dsmil-checkpoint.ipynb code/.ipynb_checkpoints/train_dsmil-checkpoint.ipynb testing/.ipynb_checkpoints/train_dsmil-checkpoint.ipynb\n",
      "file_path:  vits-checkpoint.py code/.ipynb_checkpoints/vits-checkpoint.py testing/.ipynb_checkpoints/vits-checkpoint.py\n",
      "file_path:  train_dsmil-checkpoint.py code/.ipynb_checkpoints/train_dsmil-checkpoint.py testing/.ipynb_checkpoints/train_dsmil-checkpoint.py\n",
      "file_path:  dsmil.py code/models/dsmil.py testing/models/dsmil.py\n",
      "file_path:  transmil.py code/models/transmil.py testing/models/transmil.py\n",
      "file_path:  dsmil.cpython-310.pyc code/models/__pycache__/dsmil.cpython-310.pyc testing/models/__pycache__/dsmil.cpython-310.pyc\n",
      "file_path:  __init__.py code/moco/__init__.py testing/moco/__init__.py\n",
      "file_path:  builder.py code/moco/builder.py testing/moco/builder.py\n",
      "file_path:  loader.py code/moco/loader.py testing/moco/loader.py\n",
      "file_path:  optimizer.py code/moco/optimizer.py testing/moco/optimizer.py\n",
      "file_path:  datasets.py code/utils/datasets.py testing/utils/datasets.py\n",
      "file_path:  test_dsmil.py code/utils/test_dsmil.py testing/utils/test_dsmil.py\n",
      "file_path:  train_dsmil.py code/utils/train_dsmil.py testing/utils/train_dsmil.py\n",
      "file_path:  datasets.cpython-310.pyc code/utils/__pycache__/datasets.cpython-310.pyc testing/utils/__pycache__/datasets.cpython-310.pyc\n",
      "file_path:  train_dsmil.cpython-310.pyc code/utils/__pycache__/train_dsmil.cpython-310.pyc testing/utils/__pycache__/train_dsmil.cpython-310.pyc\n",
      "file_path:  test_dsmil.cpython-310.pyc code/utils/__pycache__/test_dsmil.cpython-310.pyc testing/utils/__pycache__/test_dsmil.cpython-310.pyc\n",
      "file_path:  datasets-checkpoint.py code/utils/.ipynb_checkpoints/datasets-checkpoint.py testing/utils/.ipynb_checkpoints/datasets-checkpoint.py\n",
      "Uploaded 35 files to oncodata-sagemaker-shared\n"
     ]
    }
   ],
   "source": [
    "s3syncher.sync()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
