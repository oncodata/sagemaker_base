{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18dbc48b-b627-421f-bc37-dceda3556004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "\n",
    "S3_BUCKET_AUTO_SAVE = 'sagemaker-us-east-1-654654320282'  # Esse é um bucket padrão para salvar automaticamente dados da execução do training job. Não precisa modificar.\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "# Make sure that you are running training in the same region as your S3 bucket\n",
    "os.environ['AWS_DEFAULT_REGION'] = region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faabb89-ec11-43aa-8a46-4d7550847c06",
   "metadata": {},
   "source": [
    "| Instance Type    | GPUs                         | vCPUs | Memory (RAM) | Cost (us-east-1)   |\n",
    "|------------------|------------------------------|-------|--------------|-------------------|\n",
    "| ml.g5.4xlarge    | NVIDIA A100 Tensor Core (1x) | 16    | 64 GB        | \\$2.03 per hour   |\n",
    "| ml.p3.2xlarge    | NVIDIA V100 Tensor Core (1x) | 8     | 61 GB        | \\$3.83 per hour   |\n",
    "| ml.g4dn.12xlarge | NVIDIA T4 Tensor Core (4x)   | 48    | 192 GB       | \\$4.89 per hour   |\n",
    "| ml.p3.8xlarge    | NVIDIA V100 Tensor Core (4x) | 32    | 244 GB       | \\$14.69 per hour  |\n",
    "| ml.p3.16xlarge   | NVIDIA V100 Tensor Core (8x) | 64    | 488 GB       | \\$28.15 per hour  |\n",
    "| ml.p3dn.24xlarge | NVIDIA V100 Tensor Core (8x) | 96    | 768 GB       | \\$35.89 per hour  |\n",
    "| ml.p4d.24xlarge  | NVIDIA A100 Tensor Core (8x) | 96    | 1.1 TB       | \\$37.69 per hour  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6acc30dc-c9f4-4fe6-8aaf-98de3fb91e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalmente vamos precisar modificar apenas as variáveis contidas nesta célula para executar o nosso training job.\n",
    "user_id = \"edmundo\"  # This is used for naming your training job\n",
    "S3_BUCKET_SHARED = \"oncodata-sagemaker-shared\"\n",
    "\n",
    "# this is all for naming\n",
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"  # This can be any instance from the table above\n",
    "nodes = 1  # This is the number of nodes or instances you want to use.\n",
    "source_dir = 'code'  # specifies the local directory (relative to where the script is run) that contains the training script and any additional code or resources needed for the training job. This directory is uploaded to S3 and then copied to the SageMaker instance(s) before the training starts.\n",
    "entry_point = \"dsmil_train_job.py\"  # specifies the entry point script that will be executed when the training job starts. This script should be in the source_dir directory.\n",
    "\n",
    "job_name = f'{user_id}-{time_str}'\n",
    "output_path_auto_save = os.path.join(\"s3://\", S3_BUCKET_AUTO_SAVE, user_id, \"sagemaker-output\", date_str)  # specifies the S3 bucket where the training output will be stored. This directory is created if it does not exist.\n",
    "s3_manual_save_path = os.path.join(S3_BUCKET_SHARED, user_id, \"models\", date_str)  # Apenas um parâmetro que criei para salvar o modelo nesta pasta do S3\n",
    "\n",
    "# These are the hyperparameters that are passed to the training script. Neste exemplo não parametrizei tudo, mas outros parâmetros podem ser usados, como num-classes, learning-rate, etc.\n",
    "hyperparameters = {\"batch-size\": 32, \"epochs\": 50, \"s3-manual-save-path\": s3_manual_save_path}\n",
    "\n",
    "use_spot_instances = True  # Use spot instances to reduce cost. We recommend using it, since the wait time isn't usually long.\n",
    "max_run = 3600 * 4  # Máximo numero de segundos que esse job tem permissão para executar\n",
    "max_wait = 3600 * 4 if use_spot_instances else None  # Máxima espera por uma spot instance, fallback para on demand se não achar\n",
    "assert max_wait >= max_run  # Dá erro se não passar\n",
    "\n",
    "volume_size = 50  # Tamanho do storage das instâncias, em GB\n",
    "\n",
    "train_data = \"s3://oncodata-sagemaker-shared/roraima/features/stomach/Stomach_feats_mocov3/v1/\"  # Dados de treino a serem passados para o training job\n",
    "channels = {'train': train_data}  # Dados para serem passados ao training job. Neste caso, o valor em train será recuperado no script com os.environ['SM_CHANNEL_TRAIN'], mas convertido para um diretório local com os dados do bucket indicado. Pode-se usar qualquer nome de channel desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b40be58-66a0-4f04-8d34-a4b0829c5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nodes > 1 and instance_type in ['ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p3.16xlarge']:\n",
    "    distribution = {\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "else:\n",
    "    distribution = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be7e3eed-2b4f-49e0-ac86-9be4a6a4ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "                entry_point=entry_point,\n",
    "                source_dir=source_dir,\n",
    "                py_version='py310',\n",
    "                framework_version='2.0.1',\n",
    "                role=get_execution_role(),  # This is the role that Sagemaker assumes to perform tasks on your behalf\n",
    "                instance_count=nodes,\n",
    "                instance_type=instance_type,\n",
    "                distribution=distribution,\n",
    "                output_path=output_path_auto_save,\n",
    "                checkpoint_s3_uri=output_path_auto_save,\n",
    "                model_dir=output_path_auto_save,\n",
    "                hyperparameters=hyperparameters,\n",
    "                use_spot_instances=use_spot_instances,\n",
    "                max_run=max_run,\n",
    "                max_wait=max_wait,\n",
    "                volume_size=volume_size,\n",
    "                disable_profiler=True,  # Reduce number of logs since we don't need profiler or debugger for this training\n",
    "                debugger_hook_config=False,\n",
    "                input_mode='FastFile',  # Faz streaming do S3, usando o S3 como local filesystem e ao mesmo tempo não consumindo todo o armazenamento local\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a2a4479-9fc5-4b86-bc2e-86d8059ce5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: edmundo-21-03-2024-06-26-26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-21 06:26:27 Starting - Starting the training job...\n",
      "2024-03-21 06:26:42 Starting - Preparing the instances for training...\n",
      "2024-03-21 06:27:15 Downloading - Downloading input data...\n",
      "2024-03-21 06:27:30 Downloading - Downloading the training image..................\n",
      "2024-03-21 06:30:51 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:43,760 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:43,778 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:43,788 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:43,790 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:45,158 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (10.2.0)\u001b[0m\n",
      "\u001b[34mCollecting timm==0.4.9 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading timm-0.4.9-py3-none-any.whl.metadata (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting slideio (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading slideio-2.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.8.1.78)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 3)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning->-r requirements.txt (line 8)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->-r requirements.txt (line 9)) (1.11.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->-r requirements.txt (line 9)) (2.31.5)\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2022.8.12 (from scikit-image->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading tifffile-2024.2.12-py3-none-any.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting lazy_loader>=0.3 (from scikit-image->-r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning->-r requirements.txt (line 8)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 3)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 3)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 3)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (1.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning->-r requirements.txt (line 8)) (4.0.3)\u001b[0m\n",
      "\u001b[34mDownloading timm-0.4.9-py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.1/346.1 kB 39.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading slideio-2.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.8/49.8 MB 49.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/14.7 MB 116.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\u001b[0m\n",
      "\u001b[34mDownloading tifffile-2024.2.12-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 40.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tifffile, slideio, lazy_loader, scikit-image, timm\u001b[0m\n",
      "\u001b[34mSuccessfully installed lazy_loader-0.3 scikit-image-0.22.0 slideio-2.5.0 tifffile-2024.2.12 timm-0.4.9\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,635 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,635 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,654 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,683 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,711 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,722 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"epochs\": 50,\n",
      "        \"s3-manual-save-path\": \"oncodata-sagemaker-shared/edmundo/models/21-03-2024\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"edmundo-21-03-2024-06-26-26\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-654654320282/edmundo-21-03-2024-06-26-26/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"dsmil_train_job\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"dsmil_train_job.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":32,\"epochs\":50,\"s3-manual-save-path\":\"oncodata-sagemaker-shared/edmundo/models/21-03-2024\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=dsmil_train_job.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=dsmil_train_job\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-654654320282/edmundo-21-03-2024-06-26-26/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":32,\"epochs\":50,\"s3-manual-save-path\":\"oncodata-sagemaker-shared/edmundo/models/21-03-2024\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"edmundo-21-03-2024-06-26-26\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-654654320282/edmundo-21-03-2024-06-26-26/source/sourcedir.tar.gz\",\"module_name\":\"dsmil_train_job\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"dsmil_train_job.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--epochs\",\"50\",\"--s3-manual-save-path\",\"oncodata-sagemaker-shared/edmundo/models/21-03-2024\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mSM_HP_S3-MANUAL-SAVE-PATH=oncodata-sagemaker-shared/edmundo/models/21-03-2024\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 dsmil_train_job.py --batch-size 32 --epochs 50 --s3-manual-save-path oncodata-sagemaker-shared/edmundo/models/21-03-2024\u001b[0m\n",
      "\u001b[34m2024-03-21 06:31:50,743 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mCounts prior oversample {'Primary Tumor': 306, 'Solid Tissue Normal': 60}\u001b[0m\n",
      "\u001b[34mAfter oversample {'Primary Tumor': 306, 'Solid Tissue Normal': 300}\u001b[0m\n",
      "\u001b[34m0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExperiment 1/1\u001b[0m\n",
      "\u001b[34m0%|          | 0/50 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [1/50]  train loss: 9.1857, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m2%|▏         | 1/50 [00:35<28:56, 35.44s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [2/50]  train loss: 5.9410, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m4%|▍         | 2/50 [00:37<12:35, 15.73s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [3/50]  train loss: 5.0086, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m6%|▌         | 3/50 [00:39<07:22,  9.43s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [4/50]  train loss: 4.4500, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m8%|▊         | 4/50 [00:41<04:56,  6.44s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [5/50]  train loss: 5.0832, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m10%|█         | 5/50 [00:43<03:35,  4.80s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [6/50]  train loss: 3.8515, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m12%|█▏        | 6/50 [00:44<02:47,  3.80s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [7/50]  train loss: 4.0572, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m14%|█▍        | 7/50 [00:46<02:15,  3.16s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [8/50]  train loss: 3.5488, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m16%|█▌        | 8/50 [00:48<01:55,  2.74s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [9/50]  train loss: 3.7591, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m18%|█▊        | 9/50 [00:50<01:41,  2.47s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [10/50]  train loss: 3.0386, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m20%|██        | 10/50 [00:52<01:31,  2.28s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [11/50]  train loss: 4.0979, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m22%|██▏       | 11/50 [00:54<01:23,  2.15s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [12/50]  train loss: 2.9810, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m24%|██▍       | 12/50 [00:56<01:18,  2.06s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [13/50]  train loss: 3.3387, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m26%|██▌       | 13/50 [00:57<01:14,  2.03s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [14/50]  train loss: 3.2739, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m28%|██▊       | 14/50 [00:59<01:11,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [15/50]  train loss: 2.9909, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m30%|███       | 15/50 [01:01<01:08,  1.95s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [16/50]  train loss: 2.7750, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m32%|███▏      | 16/50 [01:03<01:05,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [17/50]  train loss: 2.7323, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m34%|███▍      | 17/50 [01:05<01:03,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [18/50]  train loss: 2.9585, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m36%|███▌      | 18/50 [01:07<01:01,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [19/50]  train loss: 3.0202, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m38%|███▊      | 19/50 [01:09<00:59,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [20/50]  train loss: 2.4894, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m40%|████      | 20/50 [01:11<00:56,  1.89s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [21/50]  train loss: 2.8392, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m42%|████▏     | 21/50 [01:13<00:54,  1.89s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [22/50]  train loss: 2.3304, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m44%|████▍     | 22/50 [01:14<00:52,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [23/50]  train loss: 3.0256, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m46%|████▌     | 23/50 [01:16<00:50,  1.87s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [24/50]  train loss: 2.2626, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m48%|████▊     | 24/50 [01:18<00:48,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [25/50]  train loss: 2.6182, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m50%|█████     | 25/50 [01:20<00:46,  1.86s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch [26/50]  train loss: 2.1191, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 26/50 [01:22<00:44,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [27/50]  train loss: 2.0946, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 27/50 [01:24<00:42,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [28/50]  train loss: 2.1007, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 28/50 [01:26<00:40,  1.86s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mEpoch [29/50]  train loss: 2.0895, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 29/50 [01:27<00:38,  1.85s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [30/50]  train loss: 1.9691, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m60%|██████    | 30/50 [01:29<00:37,  1.85s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [31/50]  train loss: 1.5339, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 31/50 [01:31<00:35,  1.85s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [32/50]  train loss: 1.6192, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 32/50 [01:33<00:33,  1.85s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [33/50]  train loss: 1.7853, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 33/50 [01:35<00:31,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [34/50]  train loss: 1.9177, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 34/50 [01:37<00:29,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [35/50]  train loss: 1.7193, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m70%|███████   | 35/50 [01:39<00:28,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [36/50]  train loss: 1.8128, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 36/50 [01:40<00:26,  1.89s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [37/50]  train loss: 2.0209, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 37/50 [01:42<00:24,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [38/50]  train loss: 1.6141, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 38/50 [01:44<00:22,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [39/50]  train loss: 1.7381, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 39/50 [01:46<00:20,  1.89s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [40/50]  train loss: 1.6853, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m80%|████████  | 40/50 [01:48<00:18,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [41/50]  train loss: 1.5272, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 41/50 [01:50<00:17,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [42/50]  train loss: 1.6331, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 42/50 [01:52<00:15,  1.93s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [43/50]  train loss: 1.6388, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 43/50 [01:54<00:13,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [44/50]  train loss: 1.6547, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 44/50 [01:56<00:11,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [45/50]  train loss: 1.4960, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m90%|█████████ | 45/50 [01:58<00:09,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [46/50]  train loss: 1.6971, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 46/50 [02:00<00:07,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [47/50]  train loss: 1.5297, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 47/50 [02:01<00:05,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [48/50]  train loss: 1.5552, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 48/50 [02:03<00:03,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [49/50]  train loss: 1.5024, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 49/50 [02:05<00:01,  1.87s/it]#033[A\u001b[0m\n",
      "\u001b[34mEpoch [50/50]  train loss: 1.5442, val loss: -1.0000, average score: -1.0000, auc: -1.0000\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [02:07<00:00,  1.87s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 50/50 [02:07<00:00,  2.55s/it]\u001b[0m\n",
      "\u001b[34mAUC 0.0 Best Epoch -1 Seed: 8531 Threshold:  -1\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [02:07<00:00, 127.49s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1/1 [02:07<00:00, 127.49s/it]\u001b[0m\n",
      "\n",
      "2024-03-21 06:34:28 Uploading - Uploading generated training model\n",
      "2024-03-21 06:34:28 Completed - Training job completed\n",
      "\u001b[34m0.9900697799248523\u001b[0m\n",
      "\u001b[34m2024-03-21 06:34:17,195 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-21 06:34:17,195 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-21 06:34:17,195 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 433\n",
      "Billable seconds: 218\n",
      "Managed Spot Training savings: 49.7%\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(channels, wait=True, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64958f77-5c83-425a-91e7-ff326d9b55ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
